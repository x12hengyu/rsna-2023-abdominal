{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-08-28T01:56:31.69007Z","iopub.status.busy":"2023-08-28T01:56:31.689593Z","iopub.status.idle":"2023-08-28T01:56:36.566798Z","shell.execute_reply":"2023-08-28T01:56:36.565838Z","shell.execute_reply.started":"2023-08-28T01:56:31.69003Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/u/x/i/xizheng/miniconda3/envs/inr/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n","  warnings.warn(_BETA_TRANSFORMS_WARNING)\n","/u/x/i/xizheng/miniconda3/envs/inr/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n","  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"]}],"source":["# import packages\n","import pdb\n","import os\n","import pickle\n","from tqdm.notebook import tqdm\n","import random\n","from tabulate import tabulate\n","\n","import cv2\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import pandas as pd\n","import pydicom\n","import matplotlib.pyplot as plt\n","import torchvision.transforms.v2 as t\n","\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from sklearn.metrics import accuracy_score, roc_auc_score\n","from torch.utils.data import Dataset, DataLoader, Subset\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.optim import Adam\n","from torchvision import models\n","from torchvision.transforms.v2 import Resize, Compose, RandomHorizontalFlip, ColorJitter, RandomAffine, RandomErasing, ToTensor"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["BASEDIR = '../rsna-2023-abdominal-trauma-detection'\n","\n","TRAIN_IMG_PATH = os.path.join(\"../rsna-2023-png/\", 'train_images')\n","TRAIN_META_PATH = os.path.join(BASEDIR, 'train_series_meta.csv')\n","TEST_IMG_PATH = os.path.join(BASEDIR, 'test_images')\n","TEST_META_PATH = os.path.join(BASEDIR, 'test_series_meta.csv')\n","\n","TRAIN_LABEL_PATH = os.path.join(BASEDIR, 'train.csv')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T01:56:49.021756Z","iopub.status.busy":"2023-08-28T01:56:49.02141Z","iopub.status.idle":"2023-08-28T01:56:49.02871Z","shell.execute_reply":"2023-08-28T01:56:49.027413Z","shell.execute_reply.started":"2023-08-28T01:56:49.021726Z"},"trusted":true},"outputs":[],"source":["def fetch_img_paths(train_img_path):\n","    img_paths = []\n","    \n","    print('Scanning directories...')\n","    for patient in tqdm(os.listdir(train_img_path)):\n","        for scan in os.listdir(os.path.join(TRAIN_IMG_PATH, patient)):\n","            scans = []\n","            for img in os.listdir(os.path.join(TRAIN_IMG_PATH, patient, scan)):\n","                scans.append(os.path.join(TRAIN_IMG_PATH, patient, scan, img))\n","            \n","            img_paths.append(scans)\n","            \n","    return img_paths"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T01:57:51.186379Z","iopub.status.busy":"2023-08-28T01:57:51.185909Z","iopub.status.idle":"2023-08-28T01:57:51.193838Z","shell.execute_reply":"2023-08-28T01:57:51.192877Z","shell.execute_reply.started":"2023-08-28T01:57:51.186338Z"},"trusted":true},"outputs":[],"source":["def select_elements_with_spacing(input_list, spacing):\n","    \n","    \"\"\"\n","    Selects elements with a specified spacing from a given list.\n","\n","    Args:\n","        input_list (list): The input list from which elements will be selected.\n","        spacing (int): The spacing between selected elements.\n","\n","    Returns:\n","        list: A list of selected elements from the input list.\n","\n","    Raises:\n","        ValueError: If the input list does not contain at least 4 * spacing elements.\n","    \"\"\"\n"," \n","    if len(input_list) < spacing * 4:\n","        raise ValueError(\"List should contain at least 4 * spacing elements.\")\n","        \n","        \n","    # We want to select elements in the middle part of the abdomen\n","    lower_bound = int(len(input_list) * 0.4)\n","    upper_bound = int(len(input_list) * 0.6)\n","\n","    spacing = (upper_bound - lower_bound) // 3\n","    \n","    # start_index = random.randint(lower_bound, upper_bound)\n","    \n","    selected_indices = [lower_bound, lower_bound + spacing, lower_bound + (2*spacing), upper_bound]\n","    \n","    selected_elements = [input_list[index] for index in selected_indices]\n","    \n","    return selected_elements"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T01:57:53.625513Z","iopub.status.busy":"2023-08-28T01:57:53.624816Z","iopub.status.idle":"2023-08-28T01:57:53.6365Z","shell.execute_reply":"2023-08-28T01:57:53.635343Z","shell.execute_reply.started":"2023-08-28T01:57:53.625477Z"},"trusted":true},"outputs":[],"source":["def standardize_pixel_array(f):\n","    \"\"\"\n","    Standardizes a DICOM pixel array by applying various transformations.\n","    \n","    Args:\n","        dicom_path (str): Path to the DICOM image file.\n","        \n","    Returns:\n","        np.ndarray: The standardized pixel array of the DICOM image.\n","    \"\"\"\n","    dicom_image = pydicom.read_file(f)\n","    pixel_array = dicom_image.pixel_array\n","    \n","    if dicom_image.PixelRepresentation == 1:\n","        bit_shift = dicom_image.BitsAllocated - dicom_image.BitsStored\n","        dtype = pixel_array.dtype \n","        new_array = (pixel_array << bit_shift).astype(dtype) >>  bit_shift\n","        pixel_array = pydicom.pixel_data_handlers.util.apply_modality_lut(new_array, dicom_image)\n","\n","    if dicom_image.PhotometricInterpretation == \"MONOCHROME1\":\n","        pixel_array = 1 - pixel_array\n","\n","    # transform to hounsfield units\n","    intercept = dicom_image.RescaleIntercept\n","    slope = dicom_image.RescaleSlope\n","    pixel_array = pixel_array * slope + intercept\n","\n","    # windowing\n","    window_center = int(dicom_image.WindowCenter)\n","    window_width = int(dicom_image.WindowWidth)\n","    img_min = window_center - window_width // 2\n","    img_max = window_center + window_width // 2\n","    pixel_array = pixel_array.copy()\n","    pixel_array[pixel_array < img_min] = img_min\n","    pixel_array[pixel_array > img_max] = img_max\n","\n","    # normalization\n","    if pixel_array.max() == pixel_array.min():\n","        pixel_array = np.zeros_like(pixel_array)  # Handle case of constant array\n","    else:\n","        pixel_array = (pixel_array - pixel_array.min()) / (pixel_array.max() - pixel_array.min())\n","\n","    return pixel_array\n","\n","def preprocess_jpeg(jpeg_path):\n","    img = cv2.imread(jpeg_path)\n","    greyscale = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)/255\n","    return greyscale"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T01:58:13.32379Z","iopub.status.busy":"2023-08-28T01:58:13.323417Z","iopub.status.idle":"2023-08-28T01:58:13.342025Z","shell.execute_reply":"2023-08-28T01:58:13.340981Z","shell.execute_reply.started":"2023-08-28T01:58:13.323738Z"},"trusted":true},"outputs":[],"source":["# dataset\n","class AbdominalData(Dataset):\n","    \"\"\"\n","    Custom dataset class for handling abdominal trauma data classification.\n","    \n","    Args:\n","        df_path (str): Path to the CSV file containing patient labels.\n","        current_fold (int): The current fold for cross-validation.\n","        num_fold (int, optional): Total number of folds for cross-validation. Default is 5.\n","    \"\"\"\n","    \n","    def __init__(self, df_path, current_fold, num_fold = 5):\n","        \n","        super().__init__()\n","        \n","        # collect all the image instance paths\n","        self.img_paths = fetch_img_paths(TRAIN_IMG_PATH)\n","                \n","        self.df = pd.read_csv(df_path)\n","        \n","        self.num_fold = num_fold\n","        self.current_fold = current_fold\n","        self.kf = KFold(n_splits=num_fold)\n","        \n","        self.transform = Compose([\n","                            RandomHorizontalFlip(),  # Randomly flip images left-right\n","                            ColorJitter(brightness=0.2),  # Randomly adjust brightness\n","                            ColorJitter(contrast=0.2),  # Randomly adjust contrast\n","                            RandomAffine(degrees=0, shear=10),  # Apply shear transformation\n","                            RandomAffine(degrees=0, scale=(0.8, 1.2)),  # Apply zoom transformation\n","                            RandomErasing(p=0.2, scale=(0.02, 0.2)), # Coarse dropout\n","                            ToTensor(),\n","                        ])\n","    \n","    def __len__(self):\n","        \"\"\"\n","        Returns the total number of samples in the dataset.\n","        \"\"\"\n","        \n","        return len(self.img_paths)\n","    \n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Retrieves a sample from the dataset by index.\n","        \n","        Args:\n","            idx (int): Index of the dataset to retrieve.\n","        \n","        Returns:\n","            dict: A dictionary containing the image data and labels for different abdominal structures.\n","        \"\"\"\n","        \n","        # sample 4 image instances\n","        dicom_images = select_elements_with_spacing(self.img_paths[idx],\n","                                                    spacing = 2)\n","        patient_id = dicom_images[0].split('/')[-3]\n","        images = []\n","        \n","        for d in dicom_images:\n","            image = preprocess_jpeg(d)\n","            images.append(image)\n","            \n","        images = np.stack(images)\n","        image = torch.tensor(images, dtype = torch.float).unsqueeze(dim = 1)\n","        \n","        image = self.transform(image).squeeze(dim = 1)\n","        \n","        label = self.df[self.df.patient_id == int(patient_id)].values[0][1:-1]\n","        \n","        # labels\n","        bowel = np.argmax(label[0:2], keepdims = True)\n","        extravasation = np.argmax(label[2:4], keepdims = True)\n","        kidney = np.argmax(label[4:7], keepdims = False)\n","        liver = np.argmax(label[7:10], keepdims = False)\n","        spleen = np.argmax(label[10:], keepdims = False)\n","        \n","        \n","        return {\n","            'image': image,\n","            'bowel': bowel,\n","            'extravasation': extravasation,\n","            'kidney': kidney,\n","            'liver': liver,\n","            'spleen': spleen,\n","        }\n","    \n","    def get_splits(self):\n","        fold_data = list(self.kf.split(self.img_paths))\n","        train_indices, val_indices = fold_data[self.current_fold]\n","\n","        train_data = self._get_subset(train_indices)\n","        val_data = self._get_subset(val_indices)\n","        \n","        return train_data, val_data\n","\n","    def _get_subset(self, indices):\n","        return Subset(self, indices)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T01:58:21.458151Z","iopub.status.busy":"2023-08-28T01:58:21.457755Z","iopub.status.idle":"2023-08-28T01:58:21.469325Z","shell.execute_reply":"2023-08-28T01:58:21.468386Z","shell.execute_reply.started":"2023-08-28T01:58:21.458121Z"},"trusted":true},"outputs":[],"source":["class MetricsCalculator:\n","    \n","    def __init__(self, mode = 'binary'):\n","        \n","        self.probabilities = []\n","        self.predictions = []\n","        self.targets = []\n","        \n","        self.mode = mode\n","    \n","    def update(self, logits, target):\n","        \"\"\"\n","        Update the metrics calculator with predicted values and corresponding targets.\n","        \n","        Args:\n","            predicted (torch.Tensor): Predicted values.\n","            target (torch.Tensor): Ground truth targets.\n","        \"\"\"\n","        if self.mode == 'binary':\n","            probabilities = torch.sigmoid(logits)\n","            predicted = (probabilities > 0.5)\n","        else:\n","            probabilities = F.softmax(logits, dim = 1)\n","            predicted = torch.argmax(probabilities, dim=1)\n","            \n","        self.probabilities.extend(probabilities.detach().cpu().numpy())\n","        self.predictions.extend(predicted.detach().cpu().numpy())\n","        self.targets.extend(target.detach().cpu().numpy())\n","    \n","    def reset(self):\n","        \"\"\"Reset the stored predictions and targets.\"\"\"\n","        \n","        self.probabilities = []\n","        self.predictions = []\n","        self.targets = []\n","    \n","    def compute_accuracy(self):\n","        return accuracy_score(self.targets, self.predictions)\n","    \n","    def compute_auc(self):\n","        \"\"\"\n","        Compute the AUC (Area Under the Curve) metric.\n","        \n","        Returns:\n","            float: AUC.\n","        \"\"\"\n","        if self.mode == 'multi':\n","            return roc_auc_score(self.targets, self.probabilities, multi_class = 'ovo', labels=[0, 1, 2])\n","    \n","        else:\n","            return roc_auc_score(self.targets, self.probabilities)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T01:58:27.815435Z","iopub.status.busy":"2023-08-28T01:58:27.815073Z","iopub.status.idle":"2023-08-28T01:58:27.827337Z","shell.execute_reply":"2023-08-28T01:58:27.826177Z","shell.execute_reply.started":"2023-08-28T01:58:27.815403Z"},"trusted":true},"outputs":[],"source":["# Model Architecure\n","class CNNModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \n","        self.input = nn.Conv2d(4, 3, kernel_size = 3)\n","        model = models.efficientnet_b0(weights = 'IMAGENET1K_V1')\n","        \n","        self.features = model.features\n","        self.avgpool = model.avgpool\n","        \n","        self.bowel = nn.Linear(1280, 1)\n","        self.extravasation = nn.Linear(1280, 1)\n","        self.kidney = nn.Linear(1280, 3)\n","        self.liver = nn.Linear(1280,3) \n","        self.spleen = nn.Linear(1280, 3)\n","    \n","    def forward(self, x):\n","        \n","        # extract features\n","        x = self.input(x)\n","        x = self.features(x)\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        \n","        # output logits\n","        bowel = self.bowel(x)\n","        extravsation = self.extravasation(x)\n","        kidney = self.kidney(x)\n","        liver = self.liver(x)\n","        spleen = self.spleen(x)\n","        \n","        return bowel, extravsation, kidney, liver, spleen"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T01:58:29.44794Z","iopub.status.busy":"2023-08-28T01:58:29.447571Z","iopub.status.idle":"2023-08-28T01:58:33.125047Z","shell.execute_reply":"2023-08-28T01:58:33.124039Z","shell.execute_reply.started":"2023-08-28T01:58:29.447909Z"},"trusted":true},"outputs":[],"source":["\n","model = CNNModel().to('cuda')"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T02:22:38.451011Z","iopub.status.busy":"2023-08-28T02:22:38.450531Z","iopub.status.idle":"2023-08-28T02:22:38.456404Z","shell.execute_reply":"2023-08-28T02:22:38.455444Z","shell.execute_reply.started":"2023-08-28T02:22:38.450972Z"},"trusted":true},"outputs":[],"source":["BATCH_SIZE = 16\n","NUM_EPOCHS = 10\n","LR = 1e-4"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T01:58:44.679737Z","iopub.status.busy":"2023-08-28T01:58:44.679357Z","iopub.status.idle":"2023-08-28T02:01:21.500172Z","shell.execute_reply":"2023-08-28T02:01:21.49915Z","shell.execute_reply.started":"2023-08-28T01:58:44.679704Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Scanning directories...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"183337aa52384b93a83c245d0dfee0f4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3147 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Scanning directories...\n"]},{"name":"stderr","output_type":"stream","text":["/u/x/i/xizheng/miniconda3/envs/inr/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"07951995435840219e9c8eb32b03e739","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3147 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Scanning directories...\n"]},{"name":"stderr","output_type":"stream","text":["/u/x/i/xizheng/miniconda3/envs/inr/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8d5daf3be89743748b551d15835e86d0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3147 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Scanning directories...\n"]},{"name":"stderr","output_type":"stream","text":["/u/x/i/xizheng/miniconda3/envs/inr/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6ffec585bf7d4103901ab1dccfb8772a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3147 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Scanning directories...\n"]},{"name":"stderr","output_type":"stream","text":["/u/x/i/xizheng/miniconda3/envs/inr/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d13781284fa04079bb6c975823693e40","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3147 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/u/x/i/xizheng/miniconda3/envs/inr/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n","  warnings.warn(\n"]}],"source":["train_data_0, val_data_0 = AbdominalData(TRAIN_LABEL_PATH, current_fold=0).get_splits()\n","train_data_1, val_data_1 = AbdominalData(TRAIN_LABEL_PATH, current_fold=1).get_splits()\n","train_data_2, val_data_2 = AbdominalData(TRAIN_LABEL_PATH, current_fold=2).get_splits()\n","train_data_3, val_data_3 = AbdominalData(TRAIN_LABEL_PATH, current_fold=3).get_splits()\n","train_data_4, val_data_4 = AbdominalData(TRAIN_LABEL_PATH, current_fold=4).get_splits()\n","\n","train_dataloader_0 = DataLoader(train_data_0,batch_size = BATCH_SIZE, shuffle = True)\n","val_dataloader_0 = DataLoader(val_data_0,batch_size = BATCH_SIZE, shuffle = False)\n","train_dataloader_1 = DataLoader(train_data_1,batch_size = BATCH_SIZE, shuffle = True)\n","val_dataloader_1 = DataLoader(val_data_1,batch_size = BATCH_SIZE, shuffle = False)\n","train_dataloader_2 = DataLoader(train_data_2,batch_size = BATCH_SIZE, shuffle = True)\n","val_dataloader_2 = DataLoader(val_data_2,batch_size = BATCH_SIZE, shuffle = False)\n","train_dataloader_3 = DataLoader(train_data_3,batch_size = BATCH_SIZE, shuffle = True)\n","val_dataloader_3 = DataLoader(val_data_3,batch_size = BATCH_SIZE, shuffle = False)\n","train_dataloader_4 = DataLoader(train_data_4,batch_size = BATCH_SIZE, shuffle = True)\n","val_dataloader_4 = DataLoader(val_data_4,batch_size = BATCH_SIZE, shuffle = False)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T02:01:21.502908Z","iopub.status.busy":"2023-08-28T02:01:21.502033Z","iopub.status.idle":"2023-08-28T02:01:21.513025Z","shell.execute_reply":"2023-08-28T02:01:21.512013Z","shell.execute_reply.started":"2023-08-28T02:01:21.502868Z"},"trusted":true},"outputs":[],"source":["optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n","bce_b = nn.BCEWithLogitsLoss(pos_weight = torch.tensor([2.0]).to('cuda'))\n","bce_e = nn.BCEWithLogitsLoss(pos_weight = torch.tensor([4.0]).to('cuda'))\n","cce = nn.CrossEntropyLoss(label_smoothing = 0.05, weight = torch.tensor([1.0, 2.0, 4.0]).to('cuda'))\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5, verbose=True)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T02:13:37.655655Z","iopub.status.busy":"2023-08-28T02:13:37.655276Z","iopub.status.idle":"2023-08-28T02:13:37.663793Z","shell.execute_reply":"2023-08-28T02:13:37.662831Z","shell.execute_reply.started":"2023-08-28T02:13:37.655623Z"},"trusted":true},"outputs":[],"source":["# initialize metrics objects\n","train_acc_bowel = MetricsCalculator('binary')\n","train_acc_extravasation = MetricsCalculator('binary')\n","train_acc_liver = MetricsCalculator('multi')\n","train_acc_kidney = MetricsCalculator('multi')\n","train_acc_spleen = MetricsCalculator('multi')\n","\n","val_acc_bowel = MetricsCalculator('binary')\n","val_acc_extravasation = MetricsCalculator('binary')\n","val_acc_liver = MetricsCalculator('multi')\n","val_acc_kidney = MetricsCalculator('multi')\n","val_acc_spleen = MetricsCalculator('multi')"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T02:22:42.800629Z","iopub.status.busy":"2023-08-28T02:22:42.80026Z","iopub.status.idle":"2023-08-28T02:41:46.83505Z","shell.execute_reply":"2023-08-28T02:41:46.834035Z","shell.execute_reply.started":"2023-08-28T02:22:42.800598Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: [1/10]\n","Fold: 0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"646481f5dd1b4e3aad568138426a08b1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/236 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"RuntimeError","evalue":"stack expects each tensor to be equal size, but got [4, 512, 512] at entry 0 and [4, 512, 761] at entry 1","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m/data/data5785/kaggle/rsna-2-5d-cnn-training-pytorch.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-1.datascience.wisc.edu/data/data5785/kaggle/rsna-2-5d-cnn-training-pytorch.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m train_dataloader, val_dataloader \u001b[39m=\u001b[39m dataloaders[epoch\u001b[39m%\u001b[39m\u001b[39m5\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-1.datascience.wisc.edu/data/data5785/kaggle/rsna-2-5d-cnn-training-pytorch.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFold: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m%\u001b[39m\u001b[39m5\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bolvi-1.datascience.wisc.edu/data/data5785/kaggle/rsna-2-5d-cnn-training-pytorch.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, batch_data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(train_dataloader)):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-1.datascience.wisc.edu/data/data5785/kaggle/rsna-2-5d-cnn-training-pytorch.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mprint\u001b[39m(batch_data)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-1.datascience.wisc.edu/data/data5785/kaggle/rsna-2-5d-cnn-training-pytorch.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     inputs \u001b[39m=\u001b[39m batch_data[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/tqdm/notebook.py:249\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[0;32m--> 249\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[1;32m    250\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    251\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m    252\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:127\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mMapping):\n\u001b[1;32m    126\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[39mreturn\u001b[39;00m elem_type({key: collate([d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem})\n\u001b[1;32m    128\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[39m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         \u001b[39mreturn\u001b[39;00m {key: collate([d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem}\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:127\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mMapping):\n\u001b[1;32m    126\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[39mreturn\u001b[39;00m elem_type({key: collate([d[key] \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m batch], collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem})\n\u001b[1;32m    128\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[39m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         \u001b[39mreturn\u001b[39;00m {key: collate([d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem}\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n","\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [4, 512, 512] at entry 0 and [4, 512, 761] at entry 1"]}],"source":["prev_val_best_loss = float('inf')\n","\n","dataloaders = [(train_dataloader_0, val_dataloader_0),\n","               (train_dataloader_1, val_dataloader_1),\n","               (train_dataloader_2, val_dataloader_2), \n","               (train_dataloader_3, val_dataloader_3),\n","               (train_dataloader_4, val_dataloader_4)]\n","\n","for epoch in range(NUM_EPOCHS):\n","    \n","    # training\n","    model.train()\n","    \n","    train_loss = 0.0\n","    val_loss = 0.0\n","    \n","    print(f'Epoch: [{epoch+1}/{NUM_EPOCHS}]')\n","    \n","    train_dataloader, val_dataloader = dataloaders[epoch%5]\n","    \n","    print(f'Fold: {epoch%5}')\n","    \n","    for batch_idx, batch_data in enumerate(tqdm(train_dataloader)):\n","        print(batch_data)\n","        inputs = batch_data['image'].to('cuda')\n","        bowel = batch_data['bowel'].to('cuda')\n","        extravasation = batch_data['extravasation'].to('cuda')\n","        liver = batch_data['liver'].to('cuda')\n","        kidney = batch_data['kidney'].to('cuda')\n","        spleen = batch_data['spleen'].to('cuda')\n","        \n","        optimizer.zero_grad()\n","        b, e, k, l, s = model(inputs)\n","        b_loss = bce_b(b, bowel.float())\n","        e_loss = bce_e(e, extravasation.float())\n","        l_loss = cce(l, liver)\n","        k_loss = cce(k, kidney)\n","        s_loss = cce(s, spleen)\n","        \n","        total_loss = b_loss + e_loss + l_loss + k_loss + s_loss\n","        total_loss.backward()\n","        \n","        optimizer.step()\n","        \n","        # calculate training metrics\n","        train_loss += total_loss.item()\n","        train_acc_bowel.update(b, bowel)\n","        train_acc_extravasation.update(e, extravasation)\n","        train_acc_liver.update(l, liver)\n","        train_acc_kidney.update(k, kidney)\n","        train_acc_spleen.update(s, spleen)\n","    \n","    train_loss = train_loss/(batch_idx+1)\n","    \n","    # validation\n","    model.eval()\n","    running_loss = 0.0\n","    \n","    for batch_idx, batch_data in enumerate(tqdm(val_dataloader)):\n","        inputs = batch_data['image'].to('cuda')\n","        bowel = batch_data['bowel'].to('cuda')\n","        extravasation = batch_data['extravasation'].to('cuda')\n","        liver = batch_data['liver'].to('cuda')\n","        kidney = batch_data['kidney'].to('cuda')\n","        spleen = batch_data['spleen'].to('cuda')\n","\n","        \n","        b, e, k, l, s = model(inputs)\n","        b_loss = bce_b(b, bowel.float())\n","        e_loss = bce_e(e, extravasation.float())\n","        l_loss = cce(l, liver)\n","        k_loss = cce(k, kidney)\n","        s_loss = cce(s, spleen)\n","        \n","        total_loss = b_loss + e_loss + l_loss + k_loss + s_loss\n","        \n","        # calculate validation metrics\n","        val_loss += total_loss.item()\n","        val_acc_bowel.update(b, bowel)\n","        val_acc_extravasation.update(e, extravasation)\n","        val_acc_liver.update(l, liver)\n","        val_acc_kidney.update(k, kidney)\n","        val_acc_spleen.update(s, spleen)\n","    \n","    \n","    val_loss = val_loss/(batch_idx+1)\n","    scheduler.step(val_loss)\n","    \n","    if val_loss < prev_val_best_loss:\n","        prev_val_best_loss = val_loss\n","        print(\"Validation Loss improved, Saving Model...\")\n","        torch.save(model, f'efficientnet_b0_{val_loss:.3f}.pth')\n","    \n","    \n","    \n","    # accuracy and auc data\n","    metrics_data = [\n","                    [\"Bowel\", \n","                        train_acc_bowel.compute_accuracy(),\n","                        val_acc_bowel.compute_accuracy(),\n","                        train_acc_bowel.compute_auc(),\n","                        val_acc_bowel.compute_auc()],\n","                    [\"Extravasation\", \n","                        train_acc_extravasation.compute_accuracy(),\n","                        val_acc_extravasation.compute_accuracy(),\n","                        train_acc_extravasation.compute_auc(),\n","                        val_acc_extravasation.compute_auc()],\n","                    [\"Liver\", \n","                        train_acc_liver.compute_accuracy(),\n","                        val_acc_liver.compute_accuracy(),\n","                        train_acc_liver.compute_auc(),\n","                        val_acc_liver.compute_auc()],\n","                    [\"Kidney\", \n","                        train_acc_kidney.compute_accuracy(),\n","                        val_acc_kidney.compute_accuracy(),\n","                        train_acc_kidney.compute_auc(),\n","                        val_acc_kidney.compute_auc()],\n","                    [\"Spleen\", \n","                        train_acc_spleen.compute_accuracy(),\n","                        val_acc_spleen.compute_accuracy(),\n","                        train_acc_spleen.compute_auc(),\n","                        val_acc_spleen.compute_auc()]\n","                ]\n","    \n","    # verbose\n","    print('')\n","    print(tabulate(metrics_data, headers=[\"\", \"Train Acc\", \"Val Acc\", \"Train AUC\", \"Val AUC\"]))\n","    \n","    print(f'\\nMean Train Loss: {train_loss:.3f}')\n","    print(f'Mean Val Loss: {val_loss:.3f}\\n')\n","    \n","    #reset metrics\n","    train_acc_bowel.reset()\n","    train_acc_extravasation.reset()\n","    train_acc_liver.reset()\n","    train_acc_kidney.reset()\n","    train_acc_spleen.reset()\n","    val_acc_bowel.reset()\n","    val_acc_extravasation.reset()\n","    val_acc_liver.reset()\n","    val_acc_kidney.reset()\n","    val_acc_spleen.reset()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
