{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-10-12T23:09:18.181235Z","iopub.status.busy":"2023-10-12T23:09:18.180325Z","iopub.status.idle":"2023-10-12T23:09:23.984658Z","shell.execute_reply":"2023-10-12T23:09:23.983751Z","shell.execute_reply.started":"2023-10-12T23:09:18.181201Z"},"trusted":true},"outputs":[],"source":["import os\n","import pickle\n","import random\n","from tqdm import tqdm\n","\n","import cv2\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, Subset, random_split\n","from torchvision.transforms import Compose, RandomHorizontalFlip, ColorJitter, RandomAffine, RandomErasing, ToTensor, Resize\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from sklearn.metrics import accuracy_score, roc_auc_score\n","\n","import numpy as np\n","import pandas as pd\n","import pydicom\n","import pdb\n","\n","import torch\n","import torch.nn as nn\n","\n","import math\n","from collections import OrderedDict\n","from functools import partial\n","from typing import Any, Callable, List, NamedTuple, Optional, Dict\n","\n","import torch\n","import torch.nn as nn\n","\n","from torchvision.ops import Conv2dNormActivation, MLP\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","from IPython.display import clear_output, display\n","import time\n","import matplotlib.pyplot as plt\n","\n","def show_img(img):\n","    plt.figure(figsize=(5, 5))\n","    plt.imshow(img, cmap=\"gray\")\n","    plt.axis(False)\n","    plt.show()\n","    clear_output(wait=True)\n","    time.sleep(0.01)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-10-12T23:09:23.987134Z","iopub.status.busy":"2023-10-12T23:09:23.986480Z","iopub.status.idle":"2023-10-12T23:09:23.993186Z","shell.execute_reply":"2023-10-12T23:09:23.992172Z","shell.execute_reply.started":"2023-10-12T23:09:23.987100Z"},"trusted":true},"outputs":[],"source":["BASEDIR = '../rsna-2023-abdominal-trauma-detection'\n","\n","TRAIN_IMG_PATH = os.path.join(BASEDIR, 'train_images')\n","TRAIN_META_PATH = os.path.join(BASEDIR, 'train_series_meta.csv')\n","TEST_IMG_PATH = os.path.join(BASEDIR, 'test_images')\n","TEST_META_PATH = os.path.join(BASEDIR, 'test_series_meta.csv')\n","\n","TRAIN_LABEL_PATH = os.path.join(BASEDIR, 'train.csv')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-10-12T23:09:23.995587Z","iopub.status.busy":"2023-10-12T23:09:23.994763Z","iopub.status.idle":"2023-10-12T23:09:24.270813Z","shell.execute_reply":"2023-10-12T23:09:24.269869Z","shell.execute_reply.started":"2023-10-12T23:09:23.995556Z"},"trusted":true},"outputs":[],"source":["from skimage.transform import resize\n","\n","def fetch_img_paths():\n","    img_paths = []\n","    \n","    for patient in tqdm(os.listdir(TRAIN_IMG_PATH)):\n","        for scan in os.listdir(os.path.join(TRAIN_IMG_PATH, patient)):\n","            scans = []\n","            for img in os.listdir(os.path.join(TRAIN_IMG_PATH, patient, scan)):\n","                scans.append(os.path.join(TRAIN_IMG_PATH, patient, scan, img))\n","            img_paths.append(scans)\n","            \n","    return img_paths\n","\n","\n","def standardize_pixel_array(dcm: pydicom.dataset.FileDataset) -> np.ndarray:\n","    \"\"\"\n","    Source : https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection/discussion/427217\n","    \"\"\"\n","    # Correct DICOM pixel_array if PixelRepresentation == 1.\n","    pixel_array = dcm.pixel_array\n","    if dcm.PixelRepresentation == 1:\n","        bit_shift = dcm.BitsAllocated - dcm.BitsStored\n","        dtype = pixel_array.dtype \n","        pixel_array = (pixel_array << bit_shift).astype(dtype) >>  bit_shift\n","#         pixel_array = pydicom.pixel_data_handlers.util.apply_modality_lut(new_array, dcm)\n","\n","    intercept = float(dcm.RescaleIntercept)\n","    slope = float(dcm.RescaleSlope)\n","    center = int(dcm.WindowCenter)\n","    width = int(dcm.WindowWidth)\n","    low = center - width / 2\n","    high = center + width / 2    \n","    \n","    pixel_array = (pixel_array * slope) + intercept\n","    pixel_array = np.clip(pixel_array, low, high)\n","\n","    return pixel_array\n","\n","def dcm_read(f):\n","    dicom = pydicom.dcmread(f)\n","\n","    img = standardize_pixel_array(dicom)\n","    img = (img - img.min()) / (img.max() - img.min() + 1e-6)\n","\n","    if dicom.PhotometricInterpretation == \"MONOCHROME1\":\n","        img = 1 - img\n","    \n","    img = resize(img, (512, 512), anti_aliasing=True) # sklearn image resize\n","\n","    return img"]},{"cell_type":"markdown","metadata":{},"source":["## Dataloader"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-10-12T23:09:24.273717Z","iopub.status.busy":"2023-10-12T23:09:24.273170Z","iopub.status.idle":"2023-10-12T23:09:24.285167Z","shell.execute_reply":"2023-10-12T23:09:24.284258Z","shell.execute_reply.started":"2023-10-12T23:09:24.273679Z"},"trusted":true},"outputs":[],"source":["import torch\n","\n","def interpolate_channels(img_tensor):\n","    # Get the current number of channels\n","    C, H, W = img_tensor.shape\n","\n","    # Initialize the output tensor\n","    output = torch.zeros((80, H, W))\n","\n","    # Handle the edge case when C is 1\n","    if C == 1:\n","        for i in range(80):\n","            output[i] = img_tensor[0]\n","        return output\n","\n","    # Handle the edge case when C is 2\n","    if C == 2:\n","        for i in range(40):\n","            output[i] = img_tensor[0]\n","        for i in range(40, 80):\n","            output[i] = img_tensor[1]\n","        return output\n","\n","    # If channels are already 80 or more, return the original image\n","    if C >= 80:\n","        return img_tensor\n","\n","    # Set the first and last channels\n","    output[0] = img_tensor[0]\n","    output[79] = img_tensor[-1]\n","\n","    # Calculate the step for even spacing\n","    step = 78 / (C - 2)\n","\n","    # Evenly space the remaining original channels in the range 1-78\n","    for i in range(1, C - 1):\n","        output[int(1 + i * step)] = img_tensor[i]\n","\n","    # Perform linear interpolation\n","    for i in range(1, 79):\n","        if output[i].sum() == 0:\n","            left = i - 1\n","            right = i + 1\n","            while output[left].sum() == 0:\n","                left -= 1\n","            while output[right].sum() == 0:\n","                right += 1\n","\n","            alpha = (i - left) / (right - left)\n","\n","            output[i] = (1 - alpha) * output[left] + alpha * output[right]\n","\n","    return output\n","\n","\n","class AbdominalTestData(Dataset):\n","    def __init__(self):\n","        super().__init__()\n","        self.img_paths = fetch_img_paths()\n","                \n","    def __len__(self):\n","        return len(self.img_paths)\n","    \n","    def __getitem__(self, idx):\n","        dicom_images = self.img_paths[idx]\n","        dicom_images = sorted(dicom_images)\n","\n","        patient_id = int(dicom_images[0].split('/')[-3])\n","        series_id = int(dicom_images[0].split('/')[-2])\n","        \n","        images = []\n","        for d in dicom_images:\n","            image = dcm_read(d)\n","            # show_img(image)\n","            images.append(image)\n","        \n","        images = np.stack(images)\n","        image = torch.tensor(images, dtype = torch.float32)\n","        image = interpolate_channels(image)\n","        center_idx = image.shape[0] // 2\n","        image = image[center_idx-40:center_idx+40:2]\n","                                \n","        return image, {\n","            'patient_id': patient_id,\n","            'series_id': series_id\n","        }"]},{"cell_type":"markdown","metadata":{},"source":["## Inference Net"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-10-12T23:09:24.287519Z","iopub.status.busy":"2023-10-12T23:09:24.286828Z","iopub.status.idle":"2023-10-12T23:09:24.324541Z","shell.execute_reply":"2023-10-12T23:09:24.323476Z","shell.execute_reply.started":"2023-10-12T23:09:24.287400Z"},"trusted":true},"outputs":[],"source":["class ConvStemConfig(NamedTuple):\n","    out_channels: int\n","    kernel_size: int\n","    stride: int\n","    norm_layer: Callable[..., nn.Module] = nn.BatchNorm2d\n","    activation_layer: Callable[..., nn.Module] = nn.ReLU\n","\n","\n","class MLPBlock(MLP):\n","\n","    def __init__(self, in_dim: int, mlp_dim: int, dropout: float):\n","        super().__init__(in_dim, [mlp_dim, in_dim], activation_layer=nn.GELU, inplace=None, dropout=dropout)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear):\n","                nn.init.xavier_uniform_(m.weight)\n","                if m.bias is not None:\n","                    nn.init.normal_(m.bias, std=1e-6)\n","\n","    def _load_from_state_dict(\n","        self,\n","        state_dict,\n","        prefix,\n","        local_metadata,\n","        strict,\n","        missing_keys,\n","        unexpected_keys,\n","        error_msgs,\n","    ):\n","        version = local_metadata.get(\"version\", None)\n","\n","        if version is None or version < 2:\n","            # Replacing legacy MLPBlock with MLP. See https://github.com/pytorch/vision/pull/6053\n","            for i in range(2):\n","                for type in [\"weight\", \"bias\"]:\n","                    old_key = f\"{prefix}linear_{i+1}.{type}\"\n","                    new_key = f\"{prefix}{3*i}.{type}\"\n","                    if old_key in state_dict:\n","                        state_dict[new_key] = state_dict.pop(old_key)\n","\n","        super()._load_from_state_dict(\n","            state_dict,\n","            prefix,\n","            local_metadata,\n","            strict,\n","            missing_keys,\n","            unexpected_keys,\n","            error_msgs,\n","        )\n","\n","\n","class EncoderBlock(nn.Module):\n","    \"\"\"Transformer encoder block.\"\"\"\n","\n","    def __init__(\n","        self,\n","        num_heads: int,\n","        hidden_dim: int,\n","        mlp_dim: int,\n","        dropout: float,\n","        attention_dropout: float,\n","        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n","    ):\n","        super().__init__()\n","        self.num_heads = num_heads\n","\n","        # Attention block\n","        self.ln_1 = norm_layer(hidden_dim)\n","        self.self_attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=attention_dropout, batch_first=True)\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # MLP block\n","        self.ln_2 = norm_layer(hidden_dim)\n","        self.mlp = MLPBlock(hidden_dim, mlp_dim, dropout)\n","\n","    def forward(self, input: torch.Tensor):\n","        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n","        x = self.ln_1(input)\n","        x, _ = self.self_attention(query=x, key=x, value=x, need_weights=False)\n","        x = self.dropout(x)\n","        x = x + input\n","\n","        y = self.ln_2(x)\n","        y = self.mlp(y)\n","        return x + y\n","\n","\n","class Encoder(nn.Module):\n","    \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n","\n","    def __init__(\n","        self,\n","        seq_length: int,\n","        num_layers: int,\n","        num_heads: int,\n","        hidden_dim: int,\n","        mlp_dim: int,\n","        dropout: float,\n","        attention_dropout: float,\n","        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n","    ):\n","        super().__init__()\n","        # Note that batch_size is on the first dim because\n","        # we have batch_first=True in nn.MultiAttention() by default\n","        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_dim).normal_(std=0.02))  # from BERT\n","        self.dropout = nn.Dropout(dropout)\n","        layers: OrderedDict[str, nn.Module] = OrderedDict()\n","        for i in range(num_layers):\n","            layers[f\"encoder_layer_{i}\"] = EncoderBlock(\n","                num_heads,\n","                hidden_dim,\n","                mlp_dim,\n","                dropout,\n","                attention_dropout,\n","                norm_layer,\n","            )\n","        self.layers = nn.Sequential(layers)\n","        self.ln = norm_layer(hidden_dim)\n","\n","    def forward(self, input: torch.Tensor):\n","        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n","        input = input + self.pos_embedding\n","        return self.ln(self.layers(self.dropout(input)))\n","\n","\n","class VisionTransformer(nn.Module):\n","    \"\"\"Vision Transformer as per https://arxiv.org/abs/2010.11929.\"\"\"\n","\n","    def __init__(\n","        self,\n","        image_size: int,\n","        patch_size: int,\n","        num_layers: int,\n","        num_heads: int,\n","        hidden_dim: int,\n","        mlp_dim: int,\n","        dropout: float = 0.0,\n","        attention_dropout: float = 0.0,\n","        num_classes: int = 1000,\n","        representation_size: Optional[int] = None,\n","        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n","        conv_stem_configs: Optional[List[ConvStemConfig]] = None,\n","    ):\n","        super().__init__()\n","        torch._assert(image_size % patch_size == 0, \"Input shape indivisible by patch size!\")\n","        self.image_size = image_size\n","        self.patch_size = patch_size\n","        self.hidden_dim = hidden_dim\n","        self.mlp_dim = mlp_dim\n","        self.attention_dropout = attention_dropout\n","        self.dropout = dropout\n","        self.num_classes = num_classes\n","        self.representation_size = representation_size\n","        self.norm_layer = norm_layer\n","\n","        if conv_stem_configs is not None:\n","            # As per https://arxiv.org/abs/2106.14881\n","            seq_proj = nn.Sequential()\n","            prev_channels = 3\n","            for i, conv_stem_layer_config in enumerate(conv_stem_configs):\n","                seq_proj.add_module(\n","                    f\"conv_bn_relu_{i}\",\n","                    Conv2dNormActivation(\n","                        in_channels=prev_channels,\n","                        out_channels=conv_stem_layer_config.out_channels,\n","                        kernel_size=conv_stem_layer_config.kernel_size,\n","                        stride=conv_stem_layer_config.stride,\n","                        norm_layer=conv_stem_layer_config.norm_layer,\n","                        activation_layer=conv_stem_layer_config.activation_layer,\n","                    ),\n","                )\n","                prev_channels = conv_stem_layer_config.out_channels\n","            seq_proj.add_module(\n","                \"conv_last\", nn.Conv2d(in_channels=prev_channels, out_channels=hidden_dim, kernel_size=1)\n","            )\n","            self.conv_proj: nn.Module = seq_proj\n","        else:\n","            self.conv_proj = nn.Conv2d(\n","                in_channels=16, out_channels=hidden_dim, kernel_size=patch_size, stride=patch_size\n","            )\n","\n","        seq_length = (image_size // patch_size) ** 2\n","\n","        # Add a class token\n","        # self.class_token = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n","        # seq_length += 1\n","\n","        self.encoder = Encoder(\n","            seq_length,\n","            num_layers,\n","            num_heads,\n","            hidden_dim,\n","            mlp_dim,\n","            dropout,\n","            attention_dropout,\n","            norm_layer,\n","        )\n","        self.seq_length = seq_length\n","\n","        heads_layers: OrderedDict[str, nn.Module] = OrderedDict()\n","        if representation_size is None:\n","            heads_layers[\"head\"] = nn.Linear(hidden_dim, num_classes)\n","        else:\n","            heads_layers[\"pre_logits\"] = nn.Linear(hidden_dim, representation_size)\n","            heads_layers[\"act\"] = nn.Tanh()\n","            heads_layers[\"head\"] = nn.Linear(representation_size, num_classes)\n","\n","        self.heads = nn.Sequential(heads_layers)\n","\n","        if isinstance(self.conv_proj, nn.Conv2d):\n","            # Init the patchify stem\n","            fan_in = self.conv_proj.in_channels * self.conv_proj.kernel_size[0] * self.conv_proj.kernel_size[1]\n","            nn.init.trunc_normal_(self.conv_proj.weight, std=math.sqrt(1 / fan_in))\n","            if self.conv_proj.bias is not None:\n","                nn.init.zeros_(self.conv_proj.bias)\n","        elif self.conv_proj.conv_last is not None and isinstance(self.conv_proj.conv_last, nn.Conv2d):\n","            # Init the last 1x1 conv of the conv stem\n","            nn.init.normal_(\n","                self.conv_proj.conv_last.weight, mean=0.0, std=math.sqrt(2.0 / self.conv_proj.conv_last.out_channels)\n","            )\n","            if self.conv_proj.conv_last.bias is not None:\n","                nn.init.zeros_(self.conv_proj.conv_last.bias)\n","\n","        if hasattr(self.heads, \"pre_logits\") and isinstance(self.heads.pre_logits, nn.Linear):\n","            fan_in = self.heads.pre_logits.in_features\n","            nn.init.trunc_normal_(self.heads.pre_logits.weight, std=math.sqrt(1 / fan_in))\n","            nn.init.zeros_(self.heads.pre_logits.bias)\n","\n","        if isinstance(self.heads.head, nn.Linear):\n","            nn.init.zeros_(self.heads.head.weight)\n","            nn.init.zeros_(self.heads.head.bias)\n","\n","    def _process_input(self, x: torch.Tensor) -> torch.Tensor:\n","        n, c, h, w = x.shape\n","        p = self.patch_size\n","        torch._assert(h == self.image_size, \"Wrong image height!\")\n","        torch._assert(w == self.image_size, \"Wrong image width!\")\n","        n_h = h // p\n","        n_w = w // p\n","\n","        # (n, c, h, w) -> (n, hidden_dim, n_h, n_w)\n","        x = self.conv_proj(x)\n","        # (n, hidden_dim, n_h, n_w) -> (n, hidden_dim, (n_h * n_w))\n","        x = x.reshape(n, self.hidden_dim, n_h * n_w)\n","\n","        # (n, hidden_dim, (n_h * n_w)) -> (n, (n_h * n_w), hidden_dim)\n","        # The self attention layer expects inputs in the format (N, S, E)\n","        # where S is the source sequence length, N is the batch size, E is the\n","        # embedding dimension\n","        x = x.permute(0, 2, 1)\n","\n","        return x\n","\n","    def forward(self, x: torch.Tensor):\n","        # Reshape and permute the input tensor\n","        x = self._process_input(x)\n","        n = x.shape[0]\n","\n","        # Expand the class token to the full batch\n","        # batch_class_token = self.class_token.expand(n, -1, -1)\n","        # x = torch.cat([batch_class_token, x], dim=1)\n","\n","        x = self.encoder(x)\n","\n","        # Classifier \"token\" as used by standard language architectures\n","        # x = x[:, 0]\n","\n","        # x = self.heads(x)\n","\n","\n","        return x\n","\n","\n","\n","\n","class RSNA_model(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # [40, 512, 512]\n","        self.block_1 = nn.Sequential(\n","                nn.Conv2d(in_channels = 40, out_channels = 32, kernel_size = 5, stride = 2, padding = 2),\n","                # -> [32, 256, 256]\n","                nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 1, padding = 1),\n","                nn.MaxPool2d(kernel_size = 2, stride = 2)  # [32, 128, 128]\n","                )\n","\n","        # [32, 128, 128]\n","        self.block_2 = nn.Sequential(\n","                nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 5, stride = 2, padding = 2),\n","                # -> [32, 64, 64]\n","                nn.Conv2d(in_channels = 32, out_channels = 16, kernel_size = 3, stride = 1, padding = 1),\n","                nn.MaxPool2d(kernel_size = 2, stride = 2)  # [16, 32, 32]\n","                )\n","        self.conv_blocks = nn.Sequential(self.block_1, self.block_2)\n","\n","        self.vit = VisionTransformer(image_size = 32, patch_size = 4, num_layers = 12, num_heads = 4,\n","                                              hidden_dim = 256, mlp_dim = 1024)  # [64, 256]\n","\n","        self.flatten = nn.Flatten(1)  # [16384]\n","\n","        self.head_bowel = nn.Sequential(nn.Linear(in_features = 16384, out_features = 1024),\n","                                        nn.Linear(in_features = 1024, out_features = 2),\n","                                        nn.Softmax(dim=1))\n","        self.head_ext = nn.Sequential(nn.Linear(in_features = 16384, out_features = 1024),\n","                                      nn.Linear(in_features = 1024, out_features = 2),\n","                                      nn.Softmax(dim=1))\n","        self.head_kidney = nn.Sequential(nn.Linear(in_features = 16384, out_features = 1024),\n","                                         nn.Linear(in_features = 1024, out_features = 3),\n","                                         nn.Softmax(dim=1))\n","        self.head_liver = nn.Sequential(nn.Linear(in_features = 16384, out_features = 1024),\n","                                        nn.Linear(in_features = 1024, out_features = 3),\n","                                        nn.Softmax(dim=1))\n","        self.head_spleen = nn.Sequential(nn.Linear(in_features = 16384, out_features = 1024),\n","                                         nn.Linear(in_features = 1024, out_features = 3),\n","                                         nn.Softmax(dim=1))\n","\n","    def forward(self, x):\n","        latent_space = self.flatten(self.vit(self.conv_blocks(x)))\n","        return self.head_bowel(latent_space), \\\n","                self.head_ext(latent_space), \\\n","                self.head_kidney(latent_space), \\\n","                self.head_liver(latent_space), \\\n","                self.head_spleen(latent_space)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 3147/3147 [00:04<00:00, 729.15it/s]\n"]}],"source":["data = AbdominalTestData()\n","ten_percent = int(0.5 * len(data))\n","train_size = int(0.7 * ten_percent)\n","val_size = ten_percent - train_size\n","unused_size = len(data) - ten_percent\n","_, val_data, _ = random_split(data, [train_size, val_size, unused_size])"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/PIL/ImageFile.py:515\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m     fh \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mfileno()\n\u001b[1;32m    516\u001b[0m     fp\u001b[39m.\u001b[39mflush()\n","\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/data/data5785/kaggle/unet-inference.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m data[\u001b[39m0\u001b[39;49m]\n","\u001b[1;32m/data/data5785/kaggle/unet-inference.ipynb Cell 9\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m dicom_images:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m     image \u001b[39m=\u001b[39m dcm_read(d)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m     show_img(image)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m     images\u001b[39m.\u001b[39mappend(image)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m images \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack(images)\n","\u001b[1;32m/data/data5785/kaggle/unet-inference.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(img, cmap\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m plt\u001b[39m.\u001b[39maxis(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m plt\u001b[39m.\u001b[39;49mshow()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m clear_output(wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/matplotlib/pyplot.py:446\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[39mDisplay all open figures.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mexplicitly there.\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    445\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[0;32m--> 446\u001b[0m \u001b[39mreturn\u001b[39;00m _get_backend_mod()\u001b[39m.\u001b[39;49mshow(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/matplotlib_inline/backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[39mfor\u001b[39;00m figure_manager \u001b[39min\u001b[39;00m Gcf\u001b[39m.\u001b[39mget_all_fig_managers():\n\u001b[0;32m---> 90\u001b[0m         display(\n\u001b[1;32m     91\u001b[0m             figure_manager\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mfigure,\n\u001b[1;32m     92\u001b[0m             metadata\u001b[39m=\u001b[39;49m_fetch_figure_metadata(figure_manager\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mfigure)\n\u001b[1;32m     93\u001b[0m         )\n\u001b[1;32m     94\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     show\u001b[39m.\u001b[39m_to_draw \u001b[39m=\u001b[39m []\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/IPython/core/display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     publish_display_data(data\u001b[39m=\u001b[39mobj, metadata\u001b[39m=\u001b[39mmetadata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     format_dict, md_dict \u001b[39m=\u001b[39m \u001b[39mformat\u001b[39;49m(obj, include\u001b[39m=\u001b[39;49minclude, exclude\u001b[39m=\u001b[39;49mexclude)\n\u001b[1;32m    299\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m format_dict:\n\u001b[1;32m    300\u001b[0m         \u001b[39m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/IPython/core/formatters.py:179\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    177\u001b[0m md \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     data \u001b[39m=\u001b[39m formatter(obj)\n\u001b[1;32m    180\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     \u001b[39m# FIXME: log the exception\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[39mraise\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[39m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[39mreturn\u001b[39;00m caller(func, \u001b[39m*\u001b[39;49m(extras \u001b[39m+\u001b[39;49m args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/IPython/core/formatters.py:223\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     r \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    224\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[39m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_return(\u001b[39mNone\u001b[39;00m, args[\u001b[39m0\u001b[39m])\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/IPython/core/formatters.py:340\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 340\u001b[0m     \u001b[39mreturn\u001b[39;00m printer(obj)\n\u001b[1;32m    341\u001b[0m \u001b[39m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    342\u001b[0m method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/IPython/core/pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend_bases\u001b[39;00m \u001b[39mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 152\u001b[0m fig\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mprint_figure(bytes_io, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    153\u001b[0m data \u001b[39m=\u001b[39m bytes_io\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    154\u001b[0m \u001b[39mif\u001b[39;00m fmt \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msvg\u001b[39m\u001b[39m'\u001b[39m:\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/matplotlib/backend_bases.py:2366\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2362\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2363\u001b[0m     \u001b[39m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2364\u001b[0m     \u001b[39m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2365\u001b[0m     \u001b[39mwith\u001b[39;00m cbook\u001b[39m.\u001b[39m_setattr_cm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure, dpi\u001b[39m=\u001b[39mdpi):\n\u001b[0;32m-> 2366\u001b[0m         result \u001b[39m=\u001b[39m print_method(\n\u001b[1;32m   2367\u001b[0m             filename,\n\u001b[1;32m   2368\u001b[0m             facecolor\u001b[39m=\u001b[39;49mfacecolor,\n\u001b[1;32m   2369\u001b[0m             edgecolor\u001b[39m=\u001b[39;49medgecolor,\n\u001b[1;32m   2370\u001b[0m             orientation\u001b[39m=\u001b[39;49morientation,\n\u001b[1;32m   2371\u001b[0m             bbox_inches_restore\u001b[39m=\u001b[39;49m_bbox_inches_restore,\n\u001b[1;32m   2372\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2373\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   2374\u001b[0m     \u001b[39mif\u001b[39;00m bbox_inches \u001b[39mand\u001b[39;00m restore_bbox:\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/matplotlib/backend_bases.py:2232\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2228\u001b[0m     optional_kws \u001b[39m=\u001b[39m {  \u001b[39m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdpi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfacecolor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39medgecolor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39morientation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2230\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbbox_inches_restore\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m   2231\u001b[0m     skip \u001b[39m=\u001b[39m optional_kws \u001b[39m-\u001b[39m {\u001b[39m*\u001b[39minspect\u001b[39m.\u001b[39msignature(meth)\u001b[39m.\u001b[39mparameters}\n\u001b[0;32m-> 2232\u001b[0m     print_method \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mwraps(meth)(\u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: meth(\n\u001b[1;32m   2233\u001b[0m         \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{k: v \u001b[39mfor\u001b[39;49;00m k, v \u001b[39min\u001b[39;49;00m kwargs\u001b[39m.\u001b[39;49mitems() \u001b[39mif\u001b[39;49;00m k \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m skip}))\n\u001b[1;32m   2234\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     print_method \u001b[39m=\u001b[39m meth\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:509\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_png\u001b[39m(\u001b[39mself\u001b[39m, filename_or_obj, \u001b[39m*\u001b[39m, metadata\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, pil_kwargs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    463\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[39m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_print_pil(filename_or_obj, \u001b[39m\"\u001b[39;49m\u001b[39mpng\u001b[39;49m\u001b[39m\"\u001b[39;49m, pil_kwargs, metadata)\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:458\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[39mDraw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[39m*pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    457\u001b[0m FigureCanvasAgg\u001b[39m.\u001b[39mdraw(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 458\u001b[0m mpl\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mimsave(\n\u001b[1;32m    459\u001b[0m     filename_or_obj, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuffer_rgba(), \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49mfmt, origin\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mupper\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    460\u001b[0m     dpi\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdpi, metadata\u001b[39m=\u001b[39;49mmetadata, pil_kwargs\u001b[39m=\u001b[39;49mpil_kwargs)\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/matplotlib/image.py:1689\u001b[0m, in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1687\u001b[0m pil_kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mformat\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mformat\u001b[39m)\n\u001b[1;32m   1688\u001b[0m pil_kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mdpi\u001b[39m\u001b[39m\"\u001b[39m, (dpi, dpi))\n\u001b[0;32m-> 1689\u001b[0m image\u001b[39m.\u001b[39;49msave(fname, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpil_kwargs)\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/PIL/Image.py:2413\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2410\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39mopen(filename, \u001b[39m\"\u001b[39m\u001b[39mw+b\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2412\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2413\u001b[0m     save_handler(\u001b[39mself\u001b[39;49m, fp, filename)\n\u001b[1;32m   2414\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   2415\u001b[0m     \u001b[39mif\u001b[39;00m open_fp:\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/PIL/PngImagePlugin.py:1398\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1396\u001b[0m     _write_multiple_frames(im, fp, chunk, rawmode, default_image, append_images)\n\u001b[1;32m   1397\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1398\u001b[0m     ImageFile\u001b[39m.\u001b[39;49m_save(im, _idat(fp, chunk), [(\u001b[39m\"\u001b[39;49m\u001b[39mzip\u001b[39;49m\u001b[39m\"\u001b[39;49m, (\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m) \u001b[39m+\u001b[39;49m im\u001b[39m.\u001b[39;49msize, \u001b[39m0\u001b[39;49m, rawmode)])\n\u001b[1;32m   1400\u001b[0m \u001b[39mif\u001b[39;00m info:\n\u001b[1;32m   1401\u001b[0m     \u001b[39mfor\u001b[39;00m info_chunk \u001b[39min\u001b[39;00m info\u001b[39m.\u001b[39mchunks:\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/PIL/ImageFile.py:519\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    517\u001b[0m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[1;32m    518\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mAttributeError\u001b[39;00m, io\u001b[39m.\u001b[39mUnsupportedOperation) \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m--> 519\u001b[0m     _encode_tile(im, fp, tile, bufsize, \u001b[39mNone\u001b[39;49;00m, exc)\n\u001b[1;32m    520\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(fp, \u001b[39m\"\u001b[39m\u001b[39mflush\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    521\u001b[0m     fp\u001b[39m.\u001b[39mflush()\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/PIL/ImageFile.py:538\u001b[0m, in \u001b[0;36m_encode_tile\u001b[0;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[39mif\u001b[39;00m exc:\n\u001b[1;32m    536\u001b[0m     \u001b[39m# compress to Python file-compatible object\u001b[39;00m\n\u001b[1;32m    537\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 538\u001b[0m         errcode, data \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39;49mencode(bufsize)[\u001b[39m1\u001b[39m:]\n\u001b[1;32m    539\u001b[0m         fp\u001b[39m.\u001b[39mwrite(data)\n\u001b[1;32m    540\u001b[0m         \u001b[39mif\u001b[39;00m errcode:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["data[0]"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-10-12T23:17:42.227996Z","iopub.status.busy":"2023-10-12T23:17:42.227329Z","iopub.status.idle":"2023-10-12T23:17:44.621552Z","shell.execute_reply":"2023-10-12T23:17:44.620490Z","shell.execute_reply.started":"2023-10-12T23:17:42.227967Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 3147/3147 [00:04<00:00, 737.02it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/data/data5785/kaggle/unet-inference.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m any_injury_predictions\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m rows \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m unet(X)\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n","\u001b[1;32m/data/data5785/kaggle/unet-inference.ipynb Cell 8\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m images \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m dicom_images:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m     image \u001b[39m=\u001b[39m dcm_read(d)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m     images\u001b[39m.\u001b[39mappend(image)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m images \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack(images)\n","\u001b[1;32m/data/data5785/kaggle/unet-inference.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdcm_read\u001b[39m(f):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m     dicom \u001b[39m=\u001b[39m pydicom\u001b[39m.\u001b[39mdcmread(f)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     img \u001b[39m=\u001b[39m standardize_pixel_array(dicom)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     img \u001b[39m=\u001b[39m (img \u001b[39m-\u001b[39m img\u001b[39m.\u001b[39mmin()) \u001b[39m/\u001b[39m (img\u001b[39m.\u001b[39mmax() \u001b[39m-\u001b[39m img\u001b[39m.\u001b[39mmin() \u001b[39m+\u001b[39m \u001b[39m1e-6\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39mif\u001b[39;00m dicom\u001b[39m.\u001b[39mPhotometricInterpretation \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMONOCHROME1\u001b[39m\u001b[39m\"\u001b[39m:\n","\u001b[1;32m/data/data5785/kaggle/unet-inference.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m high \u001b[39m=\u001b[39m center \u001b[39m+\u001b[39m width \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m    \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m pixel_array \u001b[39m=\u001b[39m (pixel_array \u001b[39m*\u001b[39m slope) \u001b[39m+\u001b[39m intercept\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m pixel_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mclip(pixel_array, low, high)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bolvi-2.datascience.wisc.edu/data/data5785/kaggle/unet-inference.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mreturn\u001b[39;00m pixel_array\n","File \u001b[0;32m~/miniconda3/envs/inr/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2096\u001b[0m, in \u001b[0;36m_clip_dispatcher\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2034\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2035\u001b[0m \u001b[39m    Return selected slices of an array along given axis.\u001b[39;00m\n\u001b[1;32m   2036\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2091\u001b[0m \n\u001b[1;32m   2092\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2093\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39m\u001b[39mcompress\u001b[39m\u001b[39m'\u001b[39m, condition, axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout)\n\u001b[0;32m-> 2096\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_clip_dispatcher\u001b[39m(a, a_min, a_max, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   2097\u001b[0m     \u001b[39mreturn\u001b[39;00m (a, a_min, a_max)\n\u001b[1;32m   2100\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_clip_dispatcher)\n\u001b[1;32m   2101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclip\u001b[39m(a, a_min, a_max, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","\n","dataloader = DataLoader(val_data, batch_size=1, shuffle=False)\n","device = torch.device('cuda:2')\n","\n","unet = RSNA_model().to(device)\n","unet.load_state_dict(torch.load('./unet_sgd_sub_1.0_16_5_0.01.pth', map_location=device))\n","unet.eval()\n","\n","def calculate_any_injury(submission: pd.DataFrame) -> pd.Series:\n","    # Define the label groups\n","    binary_targets = ['bowel', 'extravasation']\n","    triple_level_targets = ['kidney', 'liver', 'spleen']\n","    all_target_categories = binary_targets + triple_level_targets\n","\n","    # Derive the any_injury label by taking the max of 1 - p(healthy) for each label group\n","    healthy_cols = [x + '_healthy' for x in all_target_categories]\n","    any_injury_predictions = (1 - submission[healthy_cols]).max(axis=1)\n","\n","    return any_injury_predictions\n","\n","\n","rows = []\n","\n","for batch, (X, y) in enumerate(dataloader):\n","    X = X.to(device)\n","    y_pred = unet(X)\n","    \n","    bowel = y_pred[0].data.cpu().numpy().tolist()[0]\n","    extravasation = y_pred[1].data.cpu().numpy().tolist()[0]\n","    kidney = y_pred[2].data.cpu().numpy().tolist()[0]\n","    liver = y_pred[3].data.cpu().numpy().tolist()[0]\n","    spleen = y_pred[4].data.cpu().numpy().tolist()[0]\n","    \n","    injury_probabilities = [\n","        bowel[1]*2, # bowel_injury\n","        extravasation[1]*6, # extravasation_injury\n","        kidney[1]*2, # kidney_low\n","        kidney[2]*4, # kidney_high\n","        liver[1]*2, # liver_low\n","        liver[2]*4, # liver_high\n","        spleen[1]*2, # spleen_low\n","        spleen[2]*4  # spleen_high\n","    ]\n","\n","    # Calculating the average\n","#     avg_injury = sum(injury_probabilities) / (2*3 + 2*4 + 2 + 6)\n","\n","#     print(bowel, extravasation, kidney, liver, spleen)\n","\n","    row = {\n","        'patient_id': y['patient_id'].item(),\n","        'series_id': y['series_id'].item(),\n","        'bowel_healthy': bowel[0],\n","        'bowel_injury': bowel[1], \n","        'extravasation_healthy': extravasation[0],\n","        'extravasation_injury': extravasation[1], \n","        'kidney_healthy': kidney[0],\n","        'kidney_low': kidney[1],\n","        'kidney_high': kidney[2], \n","        'liver_healthy': liver[0],\n","        'liver_low': liver[1],\n","        'liver_high': liver[2], \n","        'spleen_healthy': spleen[0],\n","        'spleen_low': spleen[1],\n","        'spleen_high': spleen[2],\n","#         'any_injury': avg_injury\n","    }\n","    rows.append(row)\n","    \n","submit_df = pd.DataFrame(rows)\n","submit_df['any_injury'] = calculate_any_injury(submit_df)\n","submit_df = submit_df.drop(columns=['series_id'])\n","submit_df = submit_df.groupby('patient_id').mean().reset_index()\n","\n","submit_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-12T23:17:49.921255Z","iopub.status.busy":"2023-10-12T23:17:49.920607Z","iopub.status.idle":"2023-10-12T23:17:49.927928Z","shell.execute_reply":"2023-10-12T23:17:49.926954Z","shell.execute_reply.started":"2023-10-12T23:17:49.921228Z"},"trusted":true},"outputs":[],"source":["submit_df.to_csv('submission.csv', index=False) "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
